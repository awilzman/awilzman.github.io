+++
author = "Andrew"
title = "Thoughts 3"
date = "2025-02-02"
description = "When words taste good"
tags = [
    "thought",
]
+++

Kneel before open-source
<!--more-->
A couple of months ago I envisioned the monetization of Large Language Models 
(LLMs), particularly for specialized ones to help teach, assist, learn, or 
anything else. It just so happens that I underestimated the power of open-source 
and the willingness of humans to add to the overall scientific community for 
nothing. OpenAI wanted LLMs to be difficult to create and expensive to maintain. 
The lives of data science researchers sought to undermine that, and they did. 
Because we can, time and time again, prove that solutions do not need to be 
expensive and exclusive, smart people just have to do the right thing. 
Now, I have a local LLM that I can run on my desktop with no knowledge of how to 
train one. The information and blueprints on how to do this are out there now, 
and we will soon find it everywhere. Industries from health care to retail to fast food 
already all want in on the benefits of LLMs, and they've been eagerly trying to crack the 
code that OpenAI seemed to have found. Now that they have a reasonable roadmap on 
how to create one, we will soon find ourselves dealing with these things regularly. 
<br>

I asked DeepSeek R1:14b (DSR1) about all of this by pasting in my above thoughts. 
Our conversation started circling around how these industries 
(specifically calling out my examples) will want to benefit from these methods, 
but there are ethical and quality concerns that come up. I asked DSR1 to formulate 
its own opinion and this is what it came up with: <br>


>The democratization of Large Language Models (LLMs) through open-source
>initiatives has undeniably expanded access to advanced AI tools, fostering
>a more inclusive environment for innovation. However, this shift raises
>critical concerns that must be addressed to ensure responsible and
>effective use of these technologies.
>
>Firstly, quality control is essential to maintain the reliability and
>safety of LLMs. Without robust measures, there is a risk of deploying
>models that are inaccurate or biased, potentially leading to significant
>negative impacts. Implementing clear guidelines and certification
>processes can help uphold standards and validate the trustworthiness of
>these AI tools.
>
>Secondly, education is crucial in guiding users through the ethical
>considerations and best practices associated with LLMs. As access
>broadens, providing comprehensive resources on both technical skills and
>societal implications becomes vital. This educational focus should
>encourage responsible innovation and a deeper understanding of AI's
>limitations.
>
>Lastly, governance structures are necessary to manage the decentralized
>nature of open-source projects effectively. Establishing frameworks that
>promote collaboration while ensuring accountability can help navigate
>potential conflicts and align efforts with broader societal goals.
>Inclusivity in governance is key, allowing diverse voices to contribute to
>shaping the future of AI development.
>
>In my view, while the democratization of LLMs through open-source
>initiatives is a commendable step toward innovation and inclusivity, it
>must be accompanied by meticulous attention to quality control, education,
>and governance to mitigate risks and maximize the positive impact on
>society.